{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_column', 1000)\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D,\\\n",
    "BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.losses import binary_crossentropy\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from unet.augment import random_hsv, random_shift_scale_rotate, random_horizontal_flip\n",
    "from unet.loss import dice_coeff, dice_loss, bce_dice_loss\n",
    "from unet.train_generator import train_generator\n",
    "from unet.valid_generator import valid_generator\n",
    "from unet.layers import unet_encode, unet_maxpool, unet_decode\n",
    "from unet.network import unet\n",
    "from unet.rle import run_length_encode\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting global parameters\n",
    "input_size = 256\n",
    "input_shape = (256, 256, 3)\n",
    "num_classes = 1\n",
    "start_filters = 32\n",
    "center_filters = 1024\n",
    "learning_rate = 0.0001\n",
    "max_epochs = 30\n",
    "batch_size = 4\n",
    "orig_width = 1918\n",
    "orig_height = 1280\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 32) 896         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256, 256, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 256, 256, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 256, 256, 32) 9248        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 256, 256, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 256, 256, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 32) 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 64) 18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128, 128, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 128, 128, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 64) 36928       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128, 128, 64) 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 128, 128, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 64)   0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 64, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64, 64, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 128)  147584      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 64, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 64, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 128)  0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 256)  295168      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 256)  590080      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 256)  1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 256)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 256)  0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 512)  1180160     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 512)  2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 512)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 512)  2359808     activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 512)  2048        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 512)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 8, 8, 512)    0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 8, 8, 1024)   4719616     max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 8, 8, 1024)   4096        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 8, 8, 1024)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 8, 8, 1024)   9438208     activation_11[0][0]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 8, 8, 1024)   4096        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 8, 8, 1024)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 1024) 0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16, 16, 1536) 0           activation_10[0][0]              \n",
      "                                                                 up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 512)  7078400     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 512)  2048        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 512)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 512)  2359808     activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 512)  2048        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 512)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 512)  0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 768)  0           activation_8[0][0]               \n",
      "                                                                 up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 256)  1769728     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 32, 32, 256)  1024        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 32, 32, 256)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 256)  590080      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 256)  1024        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 256)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 64, 64, 256)  0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 64, 64, 384)  0           activation_6[0][0]               \n",
      "                                                                 up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 64, 64, 128)  442496      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 64, 64, 128)  512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 64, 64, 128)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 64, 64, 128)  147584      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 64, 64, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 64, 64, 128)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 128, 128, 128 0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 128, 128, 192 0           activation_4[0][0]               \n",
      "                                                                 up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 128, 128, 64) 110656      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 128, 128, 64) 256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 128, 128, 64) 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 128, 128, 64) 36928       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 128, 128, 64) 256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 128, 128, 64) 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 256, 256, 64) 0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 256, 256, 96) 0           activation_2[0][0]               \n",
      "                                                                 up_sampling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 256, 256, 32) 27680       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 256, 256, 32) 128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_21 (Activation)      (None, 256, 256, 32) 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 256, 256, 32) 9248        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 256, 256, 32) 128         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 256, 256, 32) 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 256, 256, 1)  33          activation_22[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 31,466,753\n",
      "Trainable params: 31,454,721\n",
      "Non-trainable params: 12,032\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# summary of u-net architecture\n",
    "model = unet(input_shape = input_shape,\n",
    "            num_classes = num_classes,\n",
    "            start_filters = start_filters,\n",
    "            center_filters = center_filters,\n",
    "            learning_rate = learning_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split by 80 / 20 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set with 4070 images \n",
      "\n",
      "Validation set with 1018 images\n"
     ]
    }
   ],
   "source": [
    "traindf = pd.read_csv('train_masks.csv')\n",
    "img_id = traindf['img'].map(lambda x: x.split('.')[0])\n",
    "\n",
    "train_id, valid_id = train_test_split(img_id, test_size = 0.2, random_state = 42)\n",
    "print('Training set with {} images'.format(len(train_id)), '\\n')\n",
    "print('Validation set with {} images'.format(len(valid_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                           patience=8,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4,\n",
    "                           mode='min'),\n",
    "             ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=0.1,\n",
    "                               patience=4,\n",
    "                               verbose=1,\n",
    "                               epsilon=1e-4,\n",
    "                               mode='min'),\n",
    "             ModelCheckpoint(monitor='val_loss',\n",
    "                             filepath='weights/256_best_weights.hdf5',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             verbose=1,\n",
    "                             mode='min'),\n",
    "             TensorBoard(log_dir='logs')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the U-Net network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_generator(input_size = input_size,\n",
    "                                 train_id = train_id,\n",
    "                                 batch_size = batch_size)\n",
    "\n",
    "valid_generator = valid_generator(input_size = input_size,\n",
    "                                 valid_id = valid_id,\n",
    "                                 batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 513s - loss: 0.1486 - dice_coeff: 0.9080 - val_loss: 0.0682 - val_dice_coeff: 0.9603\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06818, saving model to weights/256_best_weights.hdf5\n",
      "Epoch 2/30\n",
      " - 480s - loss: 0.0423 - dice_coeff: 0.9757 - val_loss: 0.0260 - val_dice_coeff: 0.9859\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.06818 to 0.02599, saving model to weights/256_best_weights.hdf5\n",
      "Epoch 3/30\n",
      " - 505s - loss: 0.0260 - dice_coeff: 0.9864 - val_loss: 0.0208 - val_dice_coeff: 0.9890\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02599 to 0.02076, saving model to weights/256_best_weights.hdf5\n",
      "Epoch 4/30\n",
      " - 513s - loss: 0.0221 - dice_coeff: 0.9886 - val_loss: 0.0185 - val_dice_coeff: 0.9901\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02076 to 0.01846, saving model to weights/256_best_weights.hdf5\n",
      "Epoch 5/30\n",
      " - 475s - loss: 0.0206 - dice_coeff: 0.9894 - val_loss: 0.0168 - val_dice_coeff: 0.9910\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01846 to 0.01685, saving model to weights/256_best_weights.hdf5\n",
      "Epoch 6/30\n",
      " - 463s - loss: 0.0193 - dice_coeff: 0.9901 - val_loss: 0.0181 - val_dice_coeff: 0.9905\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01685\n",
      "Epoch 7/30\n",
      " - 491s - loss: 0.0184 - dice_coeff: 0.9905 - val_loss: 0.0145 - val_dice_coeff: 0.9922\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01685 to 0.01448, saving model to weights/256_best_weights.hdf5\n",
      "Epoch 8/30\n",
      " - 533s - loss: 0.0176 - dice_coeff: 0.9909 - val_loss: 0.0138 - val_dice_coeff: 0.9926\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01448 to 0.01383, saving model to weights/256_best_weights.hdf5\n",
      "Epoch 9/30\n",
      " - 478s - loss: 0.0172 - dice_coeff: 0.9911 - val_loss: 0.0170 - val_dice_coeff: 0.9912\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.01383\n",
      "Epoch 10/30\n",
      " - 480s - loss: 0.0166 - dice_coeff: 0.9914 - val_loss: 0.0204 - val_dice_coeff: 0.9896\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.01383\n",
      "Epoch 11/30\n",
      " - 469s - loss: 0.0162 - dice_coeff: 0.9916 - val_loss: 0.0127 - val_dice_coeff: 0.9933\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01383 to 0.01268, saving model to weights/256_best_weights.hdf5\n",
      "Epoch 12/30\n",
      " - 497s - loss: 0.0162 - dice_coeff: 0.9916 - val_loss: 0.0141 - val_dice_coeff: 0.9924\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.01268\n",
      "Epoch 13/30\n",
      " - 547s - loss: 0.0157 - dice_coeff: 0.9919 - val_loss: 0.0133 - val_dice_coeff: 0.9930\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.01268\n",
      "Epoch 14/30\n",
      " - 568s - loss: 0.0154 - dice_coeff: 0.9920 - val_loss: 0.0122 - val_dice_coeff: 0.9936\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01268 to 0.01218, saving model to weights/256_best_weights.hdf5\n",
      "Epoch 15/30\n",
      " - 540s - loss: 0.0154 - dice_coeff: 0.9920 - val_loss: 0.0119 - val_dice_coeff: 0.9937\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01218 to 0.01194, saving model to weights/256_best_weights.hdf5\n",
      "Epoch 16/30\n",
      " - 524s - loss: 0.0151 - dice_coeff: 0.9922 - val_loss: 0.0118 - val_dice_coeff: 0.9937\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01194 to 0.01184, saving model to weights/256_best_weights.hdf5\n",
      "Epoch 17/30\n",
      " - 542s - loss: 0.0151 - dice_coeff: 0.9922 - val_loss: 0.0408 - val_dice_coeff: 0.9773\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.01184\n",
      "Epoch 18/30\n",
      " - 579s - loss: 0.0148 - dice_coeff: 0.9923 - val_loss: 0.0125 - val_dice_coeff: 0.9934\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.01184\n",
      "Epoch 19/30\n",
      " - 570s - loss: 0.0144 - dice_coeff: 0.9925 - val_loss: 0.0114 - val_dice_coeff: 0.9940\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01184 to 0.01136, saving model to weights/256_best_weights.hdf5\n",
      "Epoch 20/30\n",
      " - 551s - loss: 0.0145 - dice_coeff: 0.9925 - val_loss: 0.0114 - val_dice_coeff: 0.9940\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.01136\n",
      "Epoch 21/30\n",
      " - 526s - loss: 0.0144 - dice_coeff: 0.9925 - val_loss: 0.0112 - val_dice_coeff: 0.9942\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01136 to 0.01118, saving model to weights/256_best_weights.hdf5\n",
      "Epoch 22/30\n",
      " - 531s - loss: 0.0141 - dice_coeff: 0.9927 - val_loss: 0.0113 - val_dice_coeff: 0.9942\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.01118\n",
      "Epoch 23/30\n",
      " - 513s - loss: 0.0141 - dice_coeff: 0.9926 - val_loss: 0.0110 - val_dice_coeff: 0.9943\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01118 to 0.01098, saving model to weights/256_best_weights.hdf5\n",
      "Epoch 24/30\n",
      " - 462s - loss: 0.0140 - dice_coeff: 0.9927 - val_loss: 0.0112 - val_dice_coeff: 0.9943\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01098\n",
      "Epoch 25/30\n",
      " - 458s - loss: 0.0139 - dice_coeff: 0.9927 - val_loss: 0.0110 - val_dice_coeff: 0.9943\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01098\n",
      "Epoch 26/30\n",
      " - 458s - loss: 0.0138 - dice_coeff: 0.9928 - val_loss: 0.0109 - val_dice_coeff: 0.9942\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01098 to 0.01095, saving model to weights/256_best_weights.hdf5\n",
      "Epoch 27/30\n",
      " - 465s - loss: 0.0137 - dice_coeff: 0.9928 - val_loss: 0.0109 - val_dice_coeff: 0.9944\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01095 to 0.01086, saving model to weights/256_best_weights.hdf5\n",
      "Epoch 28/30\n",
      " - 466s - loss: 0.0135 - dice_coeff: 0.9930 - val_loss: 0.0108 - val_dice_coeff: 0.9944\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01086 to 0.01084, saving model to weights/256_best_weights.hdf5\n",
      "Epoch 29/30\n",
      " - 461s - loss: 0.0136 - dice_coeff: 0.9929 - val_loss: 0.0104 - val_dice_coeff: 0.9945\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01084 to 0.01042, saving model to weights/256_best_weights.hdf5\n",
      "Epoch 30/30\n",
      " - 528s - loss: 0.0133 - dice_coeff: 0.9930 - val_loss: 0.0113 - val_dice_coeff: 0.9942\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01042\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator = train_generator,\n",
    "                    steps_per_epoch = np.ceil(float(len(train_id)) / float(batch_size)),\n",
    "                    epochs = max_epochs,\n",
    "                    verbose = 2,\n",
    "                    callbacks = callbacks,\n",
    "                    validation_data = valid_generator,\n",
    "                    validation_steps = np.ceil(float(len(valid_id)) / float(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying our output masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set with 100064 images\n"
     ]
    }
   ],
   "source": [
    "testdf = pd.read_csv('sample_submission.csv')\n",
    "img_test_id = testdf['img'].map(lambda s: s.split('.')[0])\n",
    "\n",
    "ids = []\n",
    "\n",
    "for id in img_test_id:\n",
    "    ids.append('{}.jpg'.format(id))\n",
    "print('Test set with {} images'.format(len(ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Classifying on 100064 test images with 4 batch_size...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25016/25016 [2:03:32<00:00,  2.49it/s]  \n"
     ]
    }
   ],
   "source": [
    "rles = []\n",
    "\n",
    "model.load_weights(filepath = 'weights/256_best_weights.hdf5')\n",
    "\n",
    "print('[Info] Classifying on {} test images with {} batch_size...'.format(len(img_test_id), batch_size))\n",
    "for start in tqdm(range(0, len(img_test_id), batch_size)):\n",
    "    x_batch = []\n",
    "    end = min(start + batch_size, len(img_test_id))\n",
    "    test_batch = img_test_id[start:end]\n",
    "    for id in test_batch.values:\n",
    "        img = cv2.imread('test/{}.jpg'.format(id))\n",
    "        img = cv2.resize(img, (input_size, input_size))\n",
    "        x_batch.append(img)\n",
    "    x_batch = np.array(x_batch, np.float32) / 255\n",
    "    preds = model.predict_on_batch(x_batch)\n",
    "    preds = np.squeeze(preds, axis=3)\n",
    "    for pred in preds:\n",
    "        prob = cv2.resize(pred, (orig_width, orig_height))\n",
    "        mask = prob > threshold\n",
    "        rle = run_length_encode(mask)\n",
    "        rles.append(rle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating submission file... \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating submission file...\",'\\n')\n",
    "df = pd.DataFrame({'img': ids, 'rle_mask': rles})\n",
    "df.to_csv('submit/256_submission.csv', index=False)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
