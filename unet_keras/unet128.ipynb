{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_column', 1000)\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D,\\\n",
    "BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.losses import binary_crossentropy\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from unet.augment import random_hsv, random_shift_scale_rotate, random_horizontal_flip\n",
    "from unet.loss import dice_coeff, dice_loss, bce_dice_loss\n",
    "from unet.train_generator import train_generator\n",
    "from unet.valid_generator import valid_generator\n",
    "from unet.layers import unet_encode, unet_maxpool, unet_decode\n",
    "from unet.network import unet\n",
    "from unet.rle import run_length_encode\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting global parameters\n",
    "input_size = 128\n",
    "input_shape = (128, 128, 3)\n",
    "num_classes = 1\n",
    "start_filters = 64\n",
    "center_filters = 1024\n",
    "learning_rate = 0.0001\n",
    "max_epochs = 30\n",
    "batch_size = 4\n",
    "orig_width = 1918\n",
    "orig_height = 1280\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 64) 36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128, 128, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 128, 128, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 128)  73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 128)  512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 128)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 128)  147584      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 128)  512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 64, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 128)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 256)  295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 256)  1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 256)  590080      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 256)  0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 512)  1180160     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 512)  2048        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 512)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 512)  2359808     activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 512)  2048        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 512)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 512)    0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 1024)   4719616     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 1024)   4096        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 1024)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 8, 8, 1024)   9438208     activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 8, 8, 1024)   4096        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 8, 8, 1024)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 1024) 0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16, 16, 1536) 0           activation_8[0][0]               \n",
      "                                                                 up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 512)  7078400     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 512)  2048        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 512)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 512)  2359808     activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 512)  2048        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 512)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 512)  0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 768)  0           activation_6[0][0]               \n",
      "                                                                 up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 256)  1769728     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 256)  1024        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 256)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 256)  590080      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32, 32, 256)  1024        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 32, 32, 256)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 64, 64, 256)  0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 64, 64, 384)  0           activation_4[0][0]               \n",
      "                                                                 up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 128)  442496      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 64, 64, 128)  147584      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 64, 64, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 64, 64, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 128, 128, 128 0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 128, 128, 192 0           activation_2[0][0]               \n",
      "                                                                 up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 64) 110656      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 64) 256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 64) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 64) 36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 128, 128, 64) 256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 128, 128, 64) 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 128, 128, 1)  65          activation_18[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 31,402,497\n",
      "Trainable params: 31,390,721\n",
      "Non-trainable params: 11,776\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# summary of u-net architecture\n",
    "model = unet(input_shape = input_shape,\n",
    "            num_classes = num_classes,\n",
    "            start_filters = start_filters,\n",
    "            center_filters = center_filters,\n",
    "            learning_rate = learning_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split by 80 / 20 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set with 4070 images \n",
      "\n",
      "Validation set with 1018 images\n"
     ]
    }
   ],
   "source": [
    "traindf = pd.read_csv('train_masks.csv')\n",
    "img_id = traindf['img'].map(lambda x: x.split('.')[0])\n",
    "\n",
    "train_id, valid_id = train_test_split(img_id, test_size = 0.2, random_state = 42)\n",
    "print('Training set with {} images'.format(len(train_id)), '\\n')\n",
    "print('Validation set with {} images'.format(len(valid_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                           patience=8,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4,\n",
    "                           mode='min'),\n",
    "             ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=0.1,\n",
    "                               patience=4,\n",
    "                               verbose=1,\n",
    "                               epsilon=1e-4,\n",
    "                               mode='min'),\n",
    "             ModelCheckpoint(monitor='val_loss',\n",
    "                             filepath='weights/128_best_weights.hdf5',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             verbose=1,\n",
    "                             mode='min'),\n",
    "             TensorBoard(log_dir='logs')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the U-Net network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_generator(input_size = input_size,\n",
    "                                 train_id = train_id,\n",
    "                                 batch_size = batch_size)\n",
    "\n",
    "valid_generator = valid_generator(input_size = input_size,\n",
    "                                 valid_id = valid_id,\n",
    "                                 batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 383s - loss: 0.1366 - dice_coeff: 0.9177 - val_loss: 0.0579 - val_dice_coeff: 0.9682\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05789, saving model to weights/128_best_weights.hdf5\n",
      "Epoch 2/30\n",
      " - 375s - loss: 0.0433 - dice_coeff: 0.9769 - val_loss: 0.0330 - val_dice_coeff: 0.9825\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05789 to 0.03304, saving model to weights/128_best_weights.hdf5\n",
      "Epoch 3/30\n",
      " - 375s - loss: 0.0335 - dice_coeff: 0.9826 - val_loss: 0.0249 - val_dice_coeff: 0.9870\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03304 to 0.02488, saving model to weights/128_best_weights.hdf5\n",
      "Epoch 4/30\n",
      " - 371s - loss: 0.0304 - dice_coeff: 0.9843 - val_loss: 0.0228 - val_dice_coeff: 0.9879\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02488 to 0.02280, saving model to weights/128_best_weights.hdf5\n",
      "Epoch 5/30\n",
      " - 364s - loss: 0.0288 - dice_coeff: 0.9851 - val_loss: 0.0207 - val_dice_coeff: 0.9891\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02280 to 0.02068, saving model to weights/128_best_weights.hdf5\n",
      "Epoch 6/30\n",
      " - 371s - loss: 0.0275 - dice_coeff: 0.9858 - val_loss: 0.0193 - val_dice_coeff: 0.9900\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02068 to 0.01931, saving model to weights/128_best_weights.hdf5\n",
      "Epoch 7/30\n",
      " - 370s - loss: 0.0267 - dice_coeff: 0.9862 - val_loss: 0.0205 - val_dice_coeff: 0.9889\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01931\n",
      "Epoch 8/30\n",
      " - 379s - loss: 0.0257 - dice_coeff: 0.9866 - val_loss: 0.0181 - val_dice_coeff: 0.9909\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01931 to 0.01809, saving model to weights/128_best_weights.hdf5\n",
      "Epoch 9/30\n",
      " - 371s - loss: 0.0248 - dice_coeff: 0.9871 - val_loss: 0.0181 - val_dice_coeff: 0.9910\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.01809\n",
      "Epoch 10/30\n",
      " - 376s - loss: 0.0241 - dice_coeff: 0.9874 - val_loss: 0.0165 - val_dice_coeff: 0.9915\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01809 to 0.01647, saving model to weights/128_best_weights.hdf5\n",
      "Epoch 11/30\n",
      " - 369s - loss: 0.0240 - dice_coeff: 0.9875 - val_loss: 0.0173 - val_dice_coeff: 0.9917\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.01647\n",
      "Epoch 12/30\n",
      " - 372s - loss: 0.0238 - dice_coeff: 0.9876 - val_loss: 0.0162 - val_dice_coeff: 0.9916\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01647 to 0.01615, saving model to weights/128_best_weights.hdf5\n",
      "Epoch 13/30\n",
      " - 376s - loss: 0.0233 - dice_coeff: 0.9878 - val_loss: 0.0156 - val_dice_coeff: 0.9920\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01615 to 0.01562, saving model to weights/128_best_weights.hdf5\n",
      "Epoch 14/30\n",
      " - 390s - loss: 0.0230 - dice_coeff: 0.9880 - val_loss: 0.0157 - val_dice_coeff: 0.9920\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.01562\n",
      "Epoch 15/30\n",
      " - 370s - loss: 0.0225 - dice_coeff: 0.9882 - val_loss: 0.0158 - val_dice_coeff: 0.9924\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.01562\n",
      "Epoch 16/30\n",
      " - 371s - loss: 0.0224 - dice_coeff: 0.9883 - val_loss: 0.0153 - val_dice_coeff: 0.9920\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01562 to 0.01531, saving model to weights/128_best_weights.hdf5\n",
      "Epoch 17/30\n",
      " - 381s - loss: 0.0222 - dice_coeff: 0.9884 - val_loss: 0.0151 - val_dice_coeff: 0.9927\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01531 to 0.01506, saving model to weights/128_best_weights.hdf5\n",
      "Epoch 18/30\n",
      " - 383s - loss: 0.0218 - dice_coeff: 0.9886 - val_loss: 0.0147 - val_dice_coeff: 0.9926\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01506 to 0.01466, saving model to weights/128_best_weights.hdf5\n",
      "Epoch 19/30\n",
      " - 378s - loss: 0.0215 - dice_coeff: 0.9887 - val_loss: 0.0148 - val_dice_coeff: 0.9926\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01466\n",
      "Epoch 20/30\n",
      " - 391s - loss: 0.0215 - dice_coeff: 0.9887 - val_loss: 0.0146 - val_dice_coeff: 0.9930\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01466 to 0.01459, saving model to weights/128_best_weights.hdf5\n",
      "Epoch 21/30\n",
      " - 378s - loss: 0.0214 - dice_coeff: 0.9888 - val_loss: 0.0146 - val_dice_coeff: 0.9929\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.01459\n",
      "Epoch 22/30\n",
      " - 391s - loss: 0.0212 - dice_coeff: 0.9889 - val_loss: 0.0139 - val_dice_coeff: 0.9929\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01459 to 0.01395, saving model to weights/128_best_weights.hdf5\n",
      "Epoch 23/30\n",
      " - 383s - loss: 0.0211 - dice_coeff: 0.9889 - val_loss: 0.0149 - val_dice_coeff: 0.9921\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.01395\n",
      "Epoch 24/30\n",
      " - 383s - loss: 0.0209 - dice_coeff: 0.9890 - val_loss: 0.0140 - val_dice_coeff: 0.9929\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01395\n",
      "Epoch 25/30\n",
      " - 399s - loss: 0.0206 - dice_coeff: 0.9892 - val_loss: 0.0141 - val_dice_coeff: 0.9931\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01395\n",
      "Epoch 26/30\n",
      " - 382s - loss: 0.0209 - dice_coeff: 0.9890 - val_loss: 0.0138 - val_dice_coeff: 0.9935\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01395 to 0.01381, saving model to weights/128_best_weights.hdf5\n",
      "Epoch 27/30\n",
      " - 375s - loss: 0.0206 - dice_coeff: 0.9891 - val_loss: 0.0138 - val_dice_coeff: 0.9933\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01381\n",
      "Epoch 28/30\n",
      " - 375s - loss: 0.0203 - dice_coeff: 0.9893 - val_loss: 0.0144 - val_dice_coeff: 0.9927\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01381\n",
      "Epoch 29/30\n",
      " - 377s - loss: 0.0202 - dice_coeff: 0.9894 - val_loss: 0.0144 - val_dice_coeff: 0.9930\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01381\n",
      "Epoch 30/30\n",
      " - 379s - loss: 0.0205 - dice_coeff: 0.9892 - val_loss: 0.0137 - val_dice_coeff: 0.9933\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01381 to 0.01373, saving model to weights/128_best_weights.hdf5\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator = train_generator,\n",
    "                    steps_per_epoch = np.ceil(float(len(train_id)) / float(batch_size)),\n",
    "                    epochs = max_epochs,\n",
    "                    verbose = 2,\n",
    "                    callbacks = callbacks,\n",
    "                    validation_data = valid_generator,\n",
    "                    validation_steps = np.ceil(float(len(valid_id)) / float(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying our output masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set with 100064 images\n"
     ]
    }
   ],
   "source": [
    "testdf = pd.read_csv('sample_submission.csv')\n",
    "img_test_id = testdf['img'].map(lambda s: s.split('.')[0])\n",
    "\n",
    "ids = []\n",
    "\n",
    "for id in img_test_id:\n",
    "    ids.append('{}.jpg'.format(id))\n",
    "print('Test set with {} images'.format(len(ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Classifying on 100064 test images with 4 batch_size...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25016/25016 [1:40:17<00:00,  3.79it/s]\n"
     ]
    }
   ],
   "source": [
    "rles = []\n",
    "\n",
    "model.load_weights(filepath = 'weights/128_best_weights.hdf5')\n",
    "\n",
    "print('[Info] Classifying on {} test images with {} batch_size...'.format(len(img_test_id), batch_size))\n",
    "for start in tqdm(range(0, len(img_test_id), batch_size)):\n",
    "    x_batch = []\n",
    "    end = min(start + batch_size, len(img_test_id))\n",
    "    test_batch = img_test_id[start:end]\n",
    "    for id in test_batch.values:\n",
    "        img = cv2.imread('test/{}.jpg'.format(id))\n",
    "        img = cv2.resize(img, (input_size, input_size))\n",
    "        x_batch.append(img)\n",
    "    x_batch = np.array(x_batch, np.float32) / 255\n",
    "    preds = model.predict_on_batch(x_batch)\n",
    "    preds = np.squeeze(preds, axis=3)\n",
    "    for pred in preds:\n",
    "        prob = cv2.resize(pred, (orig_width, orig_height))\n",
    "        mask = prob > threshold\n",
    "        rle = run_length_encode(mask)\n",
    "        rles.append(rle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating submission file... \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating submission file...\",'\\n')\n",
    "df = pd.DataFrame({'img': ids, 'rle_mask': rles})\n",
    "df.to_csv('submit/128_submission.csv', index=False)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
